{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import cuda\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from pinecone import Pinecone\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "import tiktoken\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.tools import BaseTool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Type, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./bigvul.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23403"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup models and Pinecone connection\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize dense and sparse models\n",
    "dense_model = SentenceTransformer(\n",
    "    'msmarco-bert-base-dot-v5',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "sparse_model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key='XXX')\n",
    "mitre_index = pc.Index('metadata-aug-mitre')\n",
    "bigvul_index = pc.Index('metadata-retrieval-bigvul')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    \"\"\"Encode text into dense and sparse vectors for hybrid search.\"\"\"\n",
    "    try:\n",
    "        # Create dense vector\n",
    "        dense_vec = dense_model.encode(text).tolist()\n",
    "        \n",
    "        # Create sparse vector\n",
    "        input_ids = tokenizer(text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = sparse_model(**input_ids.to(device))\n",
    "            sparse_vec = torch.log(1 + torch.relu(outputs.logits)) * input_ids.attention_mask.unsqueeze(-1)\n",
    "            sparse_vec = torch.max(sparse_vec, dim=1)[0].squeeze()\n",
    "        \n",
    "        # Convert to dictionary format\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()\n",
    "        values = sparse_vec[indices].cpu().tolist()\n",
    "        sparse_dict = {\"indices\": indices, \"values\": values}\n",
    "        \n",
    "        return dense_vec, sparse_dict\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Encoding failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mitre_index(cwe_id, top_k=1):\n",
    "    \"\"\"Query MITRE CWE index for weakness information.\"\"\"\n",
    "    query = f\"{cwe_id}\"\n",
    "    dense, sparse = encode(query)\n",
    "    \n",
    "    result = mitre_index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"cwe\": {\"$eq\": f\"{cwe_id}\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def query_bigvul_index(cve_id, cwe_id, top_k=5):\n",
    "    \"\"\"Query BigVul index for vulnerability information.\"\"\"\n",
    "    query = f\"Vulnerability: {cve_id} and Weakness: {cwe_id}\"\n",
    "    dense, sparse = encode(query)\n",
    "    \n",
    "    result = bigvul_index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"cwe\": {\"$eq\": cwe_id},\n",
    "            \"cve\": {\"$eq\": cve_id}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "CONTEXT_WINDOW = 128000  # Configurable context window\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "SAFETY_BUFFER = 500  # Reserve tokens for response\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=\"XXX\"\n",
    ")\n",
    "\n",
    "# Initialize tiktoken encoder for the model\n",
    "try:\n",
    "    encoding = tiktoken.encoding_for_model(MODEL_NAME)\n",
    "except KeyError:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def estimate_prompt_tokens(context: str) -> int:\n",
    "    base_prompt = \"\"\"\n",
    "    # CONTEXT #\n",
    "    You are a software engineer and software vulnerability  expert who specializes in recommending fixes for vulnerable code affected by different CWEs and CVEs.\n",
    "\n",
    "    # OBJECTIVE #\n",
    "    Your task is to recommend fixes for the provided vulnerable code. The recommendations should address the specific CWE in question and ensure that the code is secure against the identified vulnerabilities.\n",
    "\n",
    "    # STYLE #\n",
    "    Write in a technical and concise manner, providing clear and actionable steps. \n",
    "\n",
    "    # TONE #\n",
    "    Professional and technical.\n",
    "\n",
    "    # AUDIENCE #\n",
    "    The target audience is software developers and security professionals who are looking to secure their code against known vulnerabilities.\n",
    "\n",
    "    # RESPONSE FORMAT #\n",
    "    Provide a structured recommendation in the following format:\n",
    "    - Issue: [Brief description of the vulnerability]\n",
    "    - Recommendation: [Detailed steps to fix the vulnerability]\n",
    "    - Fix: [Code snippet demonstrating the fix]\n",
    "    \n",
    "    Using this context that contains extra information and previous vulnerable code examples and fixes for the CWE in question, recommend how the vulnerable code can be fixed:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_tokens = count_tokens(base_prompt)\n",
    "    context_tokens = count_tokens(context)\n",
    "    \n",
    "    return prompt_tokens + context_tokens\n",
    "\n",
    "def can_fit_in_context(code: str, context: str, context_window: int = CONTEXT_WINDOW) -> bool:\n",
    "    \"\"\"Check if code + prompt can fit in context window.\"\"\"\n",
    "    code_tokens = count_tokens(code)\n",
    "    prompt_tokens = estimate_prompt_tokens(context)\n",
    "    total_tokens = code_tokens + prompt_tokens + SAFETY_BUFFER\n",
    "    \n",
    "    return total_tokens <= context_window\n",
    "\n",
    "def calculate_optimal_chunk_size(context_window: int, context: str) -> int:\n",
    "    \"\"\"Calculate optimal chunk size based on context window and prompt size.\"\"\"\n",
    "    prompt_tokens = estimate_prompt_tokens(context)\n",
    "    available_tokens = context_window - prompt_tokens - SAFETY_BUFFER\n",
    "    \n",
    "    chunk_size_chars = int(available_tokens * 3.5)\n",
    "    \n",
    "    return max(chunk_size_chars, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a software engineer and software vulnerability  expert who specializes in recommending fixes for vulnerable code affected by different CWEs and CVEs.\"),\n",
    "    (\"human\", \"\"\"\n",
    "        # CONTEXT #\n",
    "        You are a software engineer and software vulnerability expert who specializes in recommending fixes for vulnerable code affected by different CWEs and CVEs. This includes understanding the specific vulnerabilities and their potential impacts.\n",
    "    \n",
    "        # OBJECTIVE #\n",
    "        Your task is to recommend fixes for the provided vulnerable code. The recommendations should address the specific CWE in question and ensure that the code is secure against the identified vulnerabilities.\n",
    "    \n",
    "        # STYLE #\n",
    "        Write in a technical and concise manner, providing clear and actionable steps. \n",
    "    \n",
    "        # TONE #\n",
    "        Professional and technical.\n",
    "    \n",
    "        # AUDIENCE #\n",
    "        The target audience is software developers and security professionals who are looking to secure their code against known vulnerabilities.\n",
    "    \n",
    "        # RESPONSE FORMAT #\n",
    "        Provide a structured recommendation in the following format:\n",
    "        - Issue: [Brief description of the vulnerability]\n",
    "        - Recommendation: [Detailed steps to fix the vulnerability]\n",
    "        - Fix: [Code snippet demonstrating the fix]\n",
    "        \n",
    "    \n",
    "        Using this context that contains extra information and previous vulnerable code examples and fixes for the CWE in question, recommend how the vulnerable code can be fixed:\n",
    "    \n",
    "        Context:\n",
    "        {context}\n",
    "    \"\"\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "def create_code_chunks(code: str, context: str, language: Language = Language.CPP) -> List[str]:\n",
    "    \"\"\"Create chunks with optimal size based on context window.\"\"\"\n",
    "    optimal_chunk_size = calculate_optimal_chunk_size(CONTEXT_WINDOW, context)\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "        language=language,\n",
    "        chunk_size=optimal_chunk_size,\n",
    "        chunk_overlap=int(optimal_chunk_size * 0.1)  # 10% overlap\n",
    "    )\n",
    "    docs = splitter.create_documents([code])\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "def analyze_code_directly(code: str, context: str) -> str:\n",
    "    \"\"\"Directly analyze code using LLM without chunking and generate recommendation.\"\"\"\n",
    "    combined_context = f\"\"\"\n",
    "        {context}\n",
    "\n",
    "        Vulnerable code to fix:\n",
    "        {code}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template.invoke({\"context\": combined_context})\n",
    "    result = llm.invoke(prompt)  \n",
    "\n",
    "    return result.content\n",
    "\n",
    "# Agent Tools for Chunked Processing\n",
    "class AnalyzeChunkArgs(BaseModel):\n",
    "    code: str = Field(description=\"Code chunk to analyze for vulnerabilities\")\n",
    "    context: str = Field(description=\"Context information about CWE/CVE\")\n",
    "\n",
    "class AnalyzeChunkTool(BaseTool):\n",
    "    name: str = \"analyze_chunk\"\n",
    "    description: str = \"Analyzes a specific code chunk for vulnerabilities\"\n",
    "    args_schema: Type[BaseModel] = AnalyzeChunkArgs\n",
    "    \n",
    "    def _run(self, code: str, context: str) -> str:\n",
    "        combined_context = f\"\"\"\n",
    "            {context}\n",
    "\n",
    "            Vulnerable code to fix:\n",
    "            {code}\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = prompt_template.invoke({\"context\": combined_context})\n",
    "        result = llm.invoke(prompt)\n",
    "        return result.content\n",
    "\n",
    "class SynthesizeRecommendationArgs(BaseModel):\n",
    "    chunk_outputs: List[str] = Field(description=\"Array of responses from the different code chunks\")\n",
    "    context: str = Field(description=\"Context information about CWE/CVE\")\n",
    "\n",
    "class SynthesizeRecommendationTool(BaseTool):\n",
    "    name: str = \"synthesize_recommendation\"\n",
    "    description: str = \"Synthesizes a final recommendation from multiple chunk analyses\"\n",
    "    args_schema: Type[BaseModel] = SynthesizeRecommendationArgs\n",
    "    \n",
    "    def _run(self, chunk_outputs: List[str], context: str) -> str:\n",
    "        # Filter out \"Not vulnerable\" responses\n",
    "        relevant_outputs = [output for output in chunk_outputs if \"Not vulnerable\" not in output]\n",
    "        \n",
    "        if not relevant_outputs:\n",
    "            return \"No vulnerabilities found in the provided code chunks.\"\n",
    "        \n",
    "        # Combine all relevant outputs\n",
    "        combined_outputs = \"\\n\\n\".join([f\"Chunk {i+1} Analysis:\\n{output}\" for i, output in enumerate(relevant_outputs)])\n",
    "        \n",
    "        final_prompt = f\"\"\"\n",
    "        # CONTEXT #\n",
    "        You are a software engineer and security expert who specializes in providing comprehensive recommendations for fixing vulnerabilities.\n",
    "\n",
    "        # OBJECTIVE #\n",
    "        Based on the analysis of multiple code chunks and the provided CWE/CVE context, generate a final comprehensive recommendation for fixing the vulnerability.\n",
    "\n",
    "        # STYLE #\n",
    "        Write in a technical and concise manner, providing clear and actionable steps.\n",
    "\n",
    "        # TONE #\n",
    "        Professional and technical.\n",
    "\n",
    "        # AUDIENCE #\n",
    "        Software developers and security professionals.\n",
    "\n",
    "        # RESPONSE FORMAT #\n",
    "        Provide a structured recommendation in the following format:\n",
    "        - Issue: [Brief description of the vulnerability]\n",
    "        - Recommendation: [Detailed steps to fix the vulnerability]\n",
    "        - Fix: [Code snippet demonstrating the fix]\n",
    "\n",
    "        # CWE/CVE Context #\n",
    "        {context}\n",
    "\n",
    "        # Chunk Analysis Results #\n",
    "        {combined_outputs}\n",
    "        \"\"\"\n",
    "        \n",
    "        result = llm.invoke(final_prompt.strip())\n",
    "        return result.content\n",
    "\n",
    "# Initialize agent tools\n",
    "tools = [AnalyzeChunkTool()]\n",
    "\n",
    "# Pull prompt template for agent\n",
    "agent_prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "# Create an agent executor\n",
    "agent = create_tool_calling_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=agent_prompt\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate code fix recommendation using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fix_recommendation(context, code):\n",
    "    \"\"\"Generate fix recommendation using intelligent chunking when needed.\"\"\"\n",
    "    print(f\"Code length: {len(code)} characters\")\n",
    "    print(f\"Estimated tokens: {count_tokens(code)}\")\n",
    "    \n",
    "    if can_fit_in_context(code, context):\n",
    "        print(\"✅ Code fits in context window - using direct analysis\")\n",
    "        return analyze_code_directly(code, context)\n",
    "    else:\n",
    "        print(\"❌ Code exceeds context window - using chunked analysis approach\")\n",
    "        return analyze_code_chunked(code, context)\n",
    "\n",
    "def analyze_code_chunked(code: str, context: str) -> str:\n",
    "    \"\"\"Handle chunked analysis using agents.\"\"\"\n",
    "    code_chunks = create_code_chunks(code, context)\n",
    "    print(f\"Created {len(code_chunks)} chunks\")\n",
    "    \n",
    "    output = []\n",
    "    \n",
    "    if len(code_chunks) == 1:\n",
    "        extracted_code = agent_executor.invoke({\"input\": {\n",
    "            \"code\": code_chunks[0],\n",
    "            \"context\": context\n",
    "        }})\n",
    "        output.append(extracted_code['output'])\n",
    "    else:\n",
    "        # Multiple chunks - process each\n",
    "        for i, chunk in enumerate(code_chunks):\n",
    "            print(f\"Processing chunk {i+1}/{len(code_chunks)}\")\n",
    "            extracted_code = agent_executor.invoke({\"input\": {\n",
    "                \"code\": chunk,\n",
    "                \"context\": context\n",
    "            }})\n",
    "            output.append(extracted_code['output'])\n",
    "    \n",
    "    # Generate final recommendation if we have multiple outputs\n",
    "    if len(output) > 1:\n",
    "        print(\"\\n=== SYNTHESIZING FINAL RECOMMENDATION ===\")\n",
    "        \n",
    "        final_tools = [SynthesizeRecommendationTool()]\n",
    "        \n",
    "        final_agent = create_tool_calling_agent(\n",
    "            llm=llm,\n",
    "            tools=final_tools,\n",
    "            prompt=agent_prompt\n",
    "        )\n",
    "        \n",
    "        final_agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "            agent=final_agent,\n",
    "            tools=final_tools,\n",
    "            verbose=True,\n",
    "            handle_parsing_errors=True\n",
    "        )\n",
    "        \n",
    "        final_recommendation = final_agent_executor.invoke({\"input\": {\n",
    "            \"chunk_outputs\": output,\n",
    "            \"context\": context\n",
    "        }})\n",
    "        \n",
    "        return final_recommendation['output']\n",
    "    else:\n",
    "        return output[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create context string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(cve_id, cwe_id):\n",
    "    \"\"\"Create context by querying both Pinecone indexes.\"\"\"\n",
    "    # Query MITRE CWE index for weakness information\n",
    "    try:\n",
    "        mitre_result = query_mitre_index(cwe_id)\n",
    "        cwe_context = \"\"\n",
    "        if mitre_result.matches:\n",
    "            cwe_metadata = mitre_result.matches[0].metadata\n",
    "           \n",
    "            summary = cwe_metadata.get('Summary', '')\n",
    "            context_info = cwe_metadata.get('context', '')\n",
    "            cwe_context = f\"MITRE CWE Information:\\nSummary: {summary}\\nContext: {context_info}\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying MITRE index: {e}\")\n",
    "        cwe_context = \"\"\n",
    "\n",
    "    # Query BigVul index for vulnerability information\n",
    "    try:\n",
    "        bigvul_result = query_bigvul_index(cve_id, cwe_id)\n",
    "        cve_context = \"\"\n",
    "        if bigvul_result.matches:\n",
    "            all_summaries = []\n",
    "            all_contexts = []\n",
    "            all_vuln_fixes = []\n",
    "            for match in bigvul_result.matches:\n",
    "                metadata = match.metadata\n",
    "                summary = metadata.get('Summary', '')\n",
    "                context_info = metadata.get('context', '')\n",
    "                func_before = metadata.get('func_before', '')\n",
    "                func_after = metadata.get('func_after', '')\n",
    "                if summary:\n",
    "                    all_summaries.append(summary)\n",
    "                if context_info:\n",
    "                    all_contexts.append(context_info)\n",
    "                if func_before and func_after:\n",
    "                    all_vuln_fixes.append((func_before, func_after))\n",
    "            \n",
    "            cve_context = f\"BigVul Vulnerability Information:\\n\"\n",
    "            if all_summaries:\n",
    "                cve_context += f\"Summaries: {' | '.join(all_summaries)}\\n\"\n",
    "            if all_contexts:\n",
    "                cve_context += f\"Contexts: {' | '.join(all_contexts)}\\n\"\n",
    "            if all_vuln_fixes:\n",
    "                cve_context += f\"Vulnerability and Fix Examples:\\n\"\n",
    "                for i, (vuln_code, fixed_code) in enumerate(all_vuln_fixes, 1):\n",
    "                    cve_context += f\"Example {i}:\\n\"\n",
    "                    cve_context += f\"VULNERABLE CODE:\\n{vuln_code}\\n\\n\"\n",
    "                    cve_context += f\"FIXED CODE:\\n{fixed_code}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying BigVul index: {e}\")\n",
    "        cve_context = \"\"\n",
    "    \n",
    "    context = f\"Context:\\n{cwe_context}\\n{cve_context}\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context:\\nMITRE CWE Information:\\nSummary: \\nContext: CWE ID: CWE-416\\n\\nBigVul Vulnerability Information:\\nSummaries: In Artifex Ghostscript before 9.24, attackers able to supply crafted PostScript files could use incorrect free logic in pagedevice replacement to crash the interpreter. | In Artifex Ghostscript before 9.24, attackers able to supply crafted PostScript files could use incorrect free logic in pagedevice replacement to crash the interpreter. | In Artifex Ghostscript before 9.24, attackers able to supply crafted PostScript files could use incorrect free logic in pagedevice replacement to crash the interpreter. | In Artifex Ghostscript before 9.24, attackers able to supply crafted PostScript files could use incorrect free logic in pagedevice replacement to crash the interpreter. | In Artifex Ghostscript before 9.24, attackers able to supply crafted PostScript files could use incorrect free logic in pagedevice replacement to crash the interpreter.\\nContexts: Vulnerability: CVE-2018-16541\\nWeakness: CWE-416 | Vulnerability: CVE-2018-16541\\nWeakness: CWE-416 | Vulnerability: CVE-2018-16541\\nWeakness: CWE-416 | Vulnerability: CVE-2018-16541\\nWeakness: CWE-416 | Vulnerability: CVE-2018-16541\\nWeakness: CWE-416\\nVulnerability and Fix Examples:\\nExample 1:\\nVULNERABLE CODE:\\ngs_pop_integer(gs_main_instance * minst, long *result)\\n{\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n    ref vref;\\n    int code = pop_value(i_ctx_p, &vref);\\n\\n    if (code < 0)\\n        return code;\\n    check_type_only(vref, t_integer);\\n    *result = vref.value.intval;\\n    ref_stack_pop(&o_stack, 1);\\n    return 0;\\n}\\n\\n\\nFIXED CODE:\\ngs_pop_integer(gs_main_instance * minst, long *result)\\n{\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n    ref vref;\\n    int code = pop_value(i_ctx_p, &vref);\\n\\n    if (code < 0)\\n        return code;\\n    check_type_only(vref, t_integer);\\n    *result = vref.value.intval;\\n    ref_stack_pop(&o_stack, 1);\\n    return 0;\\n}\\n\\n\\nExample 2:\\nVULNERABLE CODE:\\ngs_main_run_string_continue(gs_main_instance * minst, const char *str,\\n         uint length, int user_errors, int *pexit_code, ref * perror_object)\\n{\\n    ref rstr;\\n\\n    if (length == 0)\\n        return 0;               /* empty string signals EOF */\\n    make_const_string(&rstr, avm_foreign | a_readonly, length,\\n                      (const byte *)str);\\n    return gs_main_interpret(minst, &rstr, user_errors, pexit_code,\\n                        perror_object);\\n}\\n\\n\\nFIXED CODE:\\ngs_main_run_string_continue(gs_main_instance * minst, const char *str,\\n         uint length, int user_errors, int *pexit_code, ref * perror_object)\\n{\\n    ref rstr;\\n\\n    if (length == 0)\\n        return 0;               /* empty string signals EOF */\\n    make_const_string(&rstr, avm_foreign | a_readonly, length,\\n                      (const byte *)str);\\n    return gs_main_interpret(minst, &rstr, user_errors, pexit_code,\\n                        perror_object);\\n}\\n\\n\\nExample 3:\\nVULNERABLE CODE:\\ngs_main_dump_stack(gs_main_instance *minst, int code, ref * perror_object)\\n{\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n\\n    zflush(i_ctx_p);            /* force out buffered output */\\n    dmprintf1(minst->heap, \"\\\\nUnexpected interpreter error %d.\\\\n\", code);\\n    if (perror_object != 0) {\\n        dmputs(minst->heap, \"Error object: \");\\n        debug_print_ref(minst->heap, perror_object);\\n        dmputc(minst->heap, \\'\\\\n\\');\\n    }\\n    debug_dump_stack(minst->heap, &o_stack, \"Operand stack\");\\n    debug_dump_stack(minst->heap, &e_stack, \"Execution stack\");\\n    debug_dump_stack(minst->heap, &d_stack, \"Dictionary stack\");\\n}\\n\\n\\nFIXED CODE:\\ngs_main_dump_stack(gs_main_instance *minst, int code, ref * perror_object)\\n{\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n\\n    zflush(i_ctx_p);            /* force out buffered output */\\n    dmprintf1(minst->heap, \"\\\\nUnexpected interpreter error %d.\\\\n\", code);\\n    if (perror_object != 0) {\\n        dmputs(minst->heap, \"Error object: \");\\n        debug_print_ref(minst->heap, perror_object);\\n        dmputc(minst->heap, \\'\\\\n\\');\\n    }\\n    debug_dump_stack(minst->heap, &o_stack, \"Operand stack\");\\n    debug_dump_stack(minst->heap, &e_stack, \"Execution stack\");\\n    debug_dump_stack(minst->heap, &d_stack, \"Dictionary stack\");\\n}\\n\\n\\nExample 4:\\nVULNERABLE CODE:\\ngs_main_lib_open(gs_main_instance * minst, const char *file_name, ref * pfile)\\n{\\n    /* This is a separate procedure only to avoid tying up */\\n    /* extra stack space while running the file. */\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n#define maxfn 2048\\n    char fn[maxfn];\\n    uint len;\\n\\n    return lib_file_open(&minst->lib_path, imemory,\\n                         NULL, /* Don\\'t check permissions here, because permlist\\n                                  isn\\'t ready running init files. */\\n                          file_name, strlen(file_name), fn, maxfn, &len, pfile);\\n}\\n\\n\\nFIXED CODE:\\ngs_main_lib_open(gs_main_instance * minst, const char *file_name, ref * pfile)\\n{\\n    /* This is a separate procedure only to avoid tying up */\\n    /* extra stack space while running the file. */\\n    i_ctx_t *i_ctx_p = minst->i_ctx_p;\\n#define maxfn 2048\\n    char fn[maxfn];\\n    uint len;\\n\\n    return lib_file_open(&minst->lib_path, imemory,\\n                         NULL, /* Don\\'t check permissions here, because permlist\\n                                  isn\\'t ready running init files. */\\n                          file_name, strlen(file_name), fn, maxfn, &len, pfile);\\n}\\n\\n\\nExample 5:\\nVULNERABLE CODE:\\ngs_main_run_file(gs_main_instance * minst, const char *file_name, int user_errors, int *pexit_code, ref * perror_object)\\n{\\n    ref initial_file;\\n    int code = gs_main_run_file_open(minst, file_name, &initial_file);\\n\\n    if (code < 0)\\n        return code;\\n    return gs_main_interpret(minst, &initial_file, user_errors,\\n                        pexit_code, perror_object);\\n}\\n\\n\\nFIXED CODE:\\ngs_main_run_file(gs_main_instance * minst, const char *file_name, int user_errors, int *pexit_code, ref * perror_object)\\n{\\n    ref initial_file;\\n    int code = gs_main_run_file_open(minst, file_name, &initial_file);\\n\\n    if (code < 0)\\n        return code;\\n    return gs_main_interpret(minst, &initial_file, user_errors,\\n                        pexit_code, perror_object);\\n}\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_context(test_df['CVE ID'][1], test_df['CWE ID'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load progress if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file = './output/recommendations.csv'\n",
    "if os.path.exists(progress_file):\n",
    "    results_df = pd.read_csv(progress_file)\n",
    "    processed_ids = set(results_df['cve'].astype(str) + \"_\" + results_df['cwe'].astype(str))\n",
    "    results = results_df.to_dict('records')\n",
    "else:\n",
    "    results = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the test dataset and generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-12714_CWE-787\n",
      "Code length: 209 characters\n",
      "Estimated tokens: 49\n",
      "✅ Code fits in context window - using direct analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-16541_CWE-416\n",
      "Code length: 406 characters\n",
      "Estimated tokens: 108\n",
      "✅ Code fits in context window - using direct analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2/2 [00:11<00:00,  6.00s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(test_df.head(2).iterrows(), total=test_df.head(2).shape[0], desc=\"Processing\"):\n",
    "    current_id = f\"{row['CVE ID']}_{row['CWE ID']}\"\n",
    "\n",
    "    # Skip already processed examples\n",
    "    if current_id in processed_ids:\n",
    "        continue\n",
    "\n",
    "    # Create context string\n",
    "    context = create_context(row['CVE ID'], row['CWE ID'])\n",
    "\n",
    "    print(f\"Running for {current_id}\")\n",
    "\n",
    "    func_before = row['func_before']\n",
    "    \n",
    "    # Generate fixes\n",
    "    recommendation = generate_fix_recommendation(context, func_before)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'cve': row['CVE ID'],\n",
    "        'cwe': row['CWE ID'],\n",
    "        'context': context,\n",
    "        'func_after': row['func_after'],\n",
    "        'func_before': row['func_before'],\n",
    "        'recommendation': recommendation,\n",
    "    })\n",
    "\n",
    "    if index % 1 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(progress_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recommendation\n",
    "recommendations = pd.read_csv('./output/recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixes Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_messages = [\n",
    "    (\"system\", \"You are a software engineer and security expert who specializes in generating fixes for vulnerable code affected by different CWEs and CVEs.\"),\n",
    "    (\"human\", \"\"\"\n",
    "        # CONTEXT #\n",
    "        You are a software engineer and security expert who specializes in generating fixes for vulnerable code affected by different CWEs and CVEs.\n",
    "        \n",
    "        # OBJECTIVE #\n",
    "        Generate a fix for the given vulnerable code based on the provided context.\n",
    "        \n",
    "        # STYLE #\n",
    "        Provide the fixed code snippet only, following best practices for secure and efficient coding.\n",
    "        \n",
    "        # TONE #\n",
    "        Professional and technical.\n",
    "        \n",
    "        # AUDIENCE #\n",
    "        Software engineers and security experts.\n",
    "        \n",
    "        # RESPONSE FORMAT #\n",
    "        The response should be a single corrected code snippet without any additional explanations or comments.\n",
    "        \n",
    "        # PROMPT #\n",
    "        Based on the following vulnerable code and the given recommendation, generate a fixed version of the code:\n",
    "        {context}\n",
    "    \"\"\"),\n",
    "]\n",
    "\n",
    "fixes_prompt_template = ChatPromptTemplate.from_messages(fix_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fix(code_before, recommendation, cve, cwe):\n",
    "    combined_context = f\"\"\"\n",
    "        Vulnerable code:\n",
    "        CWE: {cwe}\n",
    "        CVE: {cve}\n",
    "\n",
    "        Code:\n",
    "        {code_before}\n",
    "\n",
    "        Recommendation:\n",
    "        {recommendation}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = fixes_prompt_template.invoke({\"context\": combined_context})\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    return result.content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file = './output/fixes.csv'\n",
    "if os.path.exists(progress_file):\n",
    "    results_df = pd.read_csv(progress_file)\n",
    "    processed_ids = set(results_df['cve'].astype(str) + \"_\" + results_df['cwe'].astype(str))\n",
    "    results = results_df.to_dict('records')\n",
    "else:\n",
    "    results = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-12714_CWE-787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-16541_CWE-416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(recommendations.iterrows(), total=recommendations.shape[0], desc=\"Processing\"):\n",
    "    current_id = f\"{row['cve']}_{row['cwe']}\"\n",
    "\n",
    "    # Skip already processed examples\n",
    "    if current_id in processed_ids:\n",
    "        continue\n",
    "\n",
    "    print(f\"Running for {current_id}\")\n",
    "\n",
    "    code_before = row['func_before']\n",
    "    \n",
    "    # Generate fixes\n",
    "    fix = generate_fix(code_before, row['recommendation'], row['cve'], row['cwe'])\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'cve': row['cve'],\n",
    "        'cwe': row['cwe'],\n",
    "        'context': row['context'],\n",
    "        'func_after': row['func_after'],\n",
    "        'func_before': row['func_before'],\n",
    "        'recommendation': row['recommendation'],\n",
    "        'fix': fix,\n",
    "    })\n",
    "\n",
    "    # Save progress every 10 examples\n",
    "    if index % 1 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(progress_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubrics = [\n",
    "    {\n",
    "        \"name\": \"Relevance\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is relevant to the identified vulnerability in both the context and the provided code snippet, and whether the suggested fix is applicable and practical within the given scenario.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Verify if the 'actual output' (recommendation) directly addresses the specific vulnerability identified in the 'input' (context), including CWE, CVE, and details from the code snippet.\",\n",
    "            \"Ensure the 'actual output' provides a solution that is applicable to both the provided context (CWE/CVE information) and the code snippet and can realistically be implemented in the given scenario.\",\n",
    "            \"Assess whether the recommendation avoids irrelevant information or suggestions that do not align with the vulnerability details or coding context.\",\n",
    "            \"Determine if the recommendation includes practical and actionable steps that align with the specific characteristics of the vulnerability, such as the type of flaw and its occurrence within the code.\",\n",
    "            \"Penalize any 'actual output' that is generic, lacks relevance to both the context and the code provided, or suggests impractical or inapplicable solutions.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Completeness\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is complete and thorough in addressing the identified vulnerability, covering all necessary steps and details required to implement the fix effectively, with reference to both the context and the code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) covers all necessary steps to fully address the vulnerability described in the 'input' (context), including CWE, CVE, and the code snippet.\",\n",
    "            \"Ensure that the recommendation provides detailed explanations and instructions for each step, leaving no ambiguity about how to implement the fix, referencing both the vulnerable code and the retrieved context.\",\n",
    "            \"Evaluate whether the recommendation considers all relevant aspects of the vulnerability, including its nature, impact, and potential variations in different coding contexts.\",\n",
    "            \"Identify any missing steps or details in the 'actual output' that could lead to incomplete implementation or residual vulnerabilities.\",\n",
    "            \"Penalize any 'actual output' that lacks comprehensiveness, omits critical details, or provides insufficient guidance for implementing a complete fix.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correctness\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is factually correct, technically accurate, and free of errors or misleading information, based on the expected output, known standards, and the provided context and code snippet for addressing the identified vulnerability.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) aligns with the correct approach to fixing the vulnerability described in the 'input' (context), including CWE, CVE, and the code snippet.\",\n",
    "            \"Verify the technical accuracy of each step in the 'actual output', ensuring they are based on sound principles, the provided code, and best practices for security fixes.\",\n",
    "            \"Identify any factual inaccuracies, technical errors, or misleading information in the 'actual output' that could lead to incorrect implementation or further vulnerabilities.\",\n",
    "            \"Evaluate if the 'actual output' correctly interprets the vulnerability details (CWE, CVE) and provides an appropriate solution based on known standards and methodologies.\",\n",
    "            \"Penalize any 'actual output' that contains factual inaccuracies, incorrect technical guidance, or misleading statements.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Identification of Vulnerable Code\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) accurately identifies the specific parts of the code that are vulnerable based on the provided context, including CWE, CVE, and the code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) clearly identifies the specific lines, functions, or code snippets in the 'input' (context) that are vulnerable.\",\n",
    "            \"Ensure the 'actual output' pinpoints the exact location of the vulnerability in the code snippet, referencing relevant details such as line numbers or specific code segments.\",\n",
    "            \"Assess whether the recommendation provides enough detail to locate the vulnerable code without ambiguity or confusion.\",\n",
    "            \"Evaluate if the 'actual output' includes all relevant parts of the code that are vulnerable, without missing any significant areas.\",\n",
    "            \"Penalize any 'actual output' that fails to identify the vulnerable code clearly, is too vague, or misidentifies non-vulnerable code as vulnerable.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Guidance\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) provides relevant, actionable code snippets and clear guidance to effectively address the identified vulnerability based on the provided context, including CWE, CVE, and code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) includes relevant and actionable code snippets that directly address the vulnerability described in the 'input' (context).\",\n",
    "            \"Ensure that the code snippets provided are syntactically correct, follow best practices, and are applicable to the coding environment specified in the context.\",\n",
    "            \"Assess whether the 'actual output' provides clear, step-by-step guidance on how to implement the code changes to fix the vulnerability.\",\n",
    "            \"Evaluate if the recommendation includes sufficient detail in the code snippets and guidance to be easily implemented by a developer without needing additional information.\",\n",
    "            \"Penalize any 'actual output' that lacks relevant code snippets, provides vague or incorrect guidance, or includes incomplete or confusing instructions.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "fixes_data = pd.read_csv('./output/fixes.csv')\n",
    "\n",
    "for index, row in fixes_data.iterrows():\n",
    "    context = create_context(row['cve'], row['cwe'])\n",
    "    \n",
    "    sample = {\n",
    "        \"cwe\": row['cwe'],\n",
    "        \"cve\": row['cve'],\n",
    "        \"code_snippet\": f\"### Vulnerable Code\\n{row['func_before']}\\n\\n\\n### Vulnerable Code Fix\\n{row['func_after']}\\n\", \n",
    "        \"context\": row['context'],\n",
    "        \"recommendation\": row['recommendation']\n",
    "    }\n",
    "    data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_text(cwe, cve, code_snippet, context):\n",
    "    return f\"\"\"\n",
    "        ### CVE-ID\n",
    "        {cve}\n",
    "\n",
    "        ### CWE-ID\n",
    "        {cwe}\n",
    "    \n",
    "        ### Vulnerable Code and Fix\n",
    "        {code_snippet}\n",
    "\n",
    "\n",
    "        ### Retrieved Data from Vector DB\n",
    "        {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = \"./output/evaluation.csv\"\n",
    "if os.path.exists(checkpoint_file):\n",
    "    df = pd.read_csv(checkpoint_file)\n",
    "    processed_keys = set(df[\"cwe_cve_idx\"])  # Track already processed cwe_cve_idx\n",
    "    results = df.to_dict('records')\n",
    "else:\n",
    "    processed_keys = set()\n",
    "    results = []\n",
    "\n",
    "def save_checkpoint(results, checkpoint_file):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(checkpoint_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, example in enumerate(tqdm(data, desc=\"Processing Examples\")):\n",
    "    cwe_cve_idx = f\"{example['cwe']}_{example['cve']}_{idx}\"  \n",
    "\n",
    "    if cwe_cve_idx in processed_keys:\n",
    "        continue \n",
    "\n",
    "    print(f\"Processing index {idx}: CWE/CVE {cwe_cve_idx}\")\n",
    "\n",
    "    example_results = {\n",
    "        \"cwe\": example[\"cwe\"],\n",
    "        \"cve\": example[\"cve\"],\n",
    "        \"cwe_cve_idx\": cwe_cve_idx,\n",
    "        \"recommendation\": example[\"recommendation\"],\n",
    "        \"code_snippet\": example[\"code_snippet\"],\n",
    "        \"context\": example[\"context\"]\n",
    "    }\n",
    "\n",
    "    for rubric in rubrics:\n",
    "        g_eval = GEval(\n",
    "            **rubric,\n",
    "            model=\"gpt-4o\",\n",
    "            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    "        )\n",
    "\n",
    "        test_case = LLMTestCase(\n",
    "            input=create_input_text(example[\"cwe\"], example[\"cve\"], example[\"code_snippet\"], example[\"context\"]),\n",
    "            actual_output=example[\"recommendation\"]\n",
    "        )\n",
    "\n",
    "        g_eval.measure(test_case)\n",
    "        example_results[f\"{rubric['name']} Score\"] = float(g_eval.score)\n",
    "\n",
    "    results.append(example_results)\n",
    "    processed_keys.add(cwe_cve_idx)\n",
    "\n",
    "    # Save checkpoint after each example is processed\n",
    "    save_checkpoint(results, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores for All Metrics:\n",
      "========================================\n",
      "Relevance Score: 0.844\n",
      "Completeness Score: 0.795\n",
      "Correctness Score: 0.782\n",
      "Identification of Vulnerable Code Score: 0.735\n",
      "Code Guidance Score: 0.843\n",
      "========================================\n",
      "Overall Average: 0.800\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores for all metrics\n",
    "def calculate_average_metrics(df):\n",
    "    score_columns = [col for col in df.columns if col.endswith(' Score')]\n",
    "    \n",
    "    print(\"Average Scores for All Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    averages = {}\n",
    "    for col in score_columns:\n",
    "        avg_score = df[col].mean()\n",
    "        averages[col] = avg_score\n",
    "        print(f\"{col}: {avg_score:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Overall Average: {sum(averages.values()) / len(averages):.3f}\")\n",
    "    \n",
    "    return averages\n",
    "\n",
    "averages = calculate_average_metrics(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Metrics Analysis:\n",
      "============================================================\n",
      "\n",
      "Relevance Score:\n",
      "  Mean: 0.844\n",
      "  Median: 0.844\n",
      "  Std Dev: 0.043\n",
      "  Min: 0.813\n",
      "  Max: 0.874\n",
      "  Count: 2\n",
      "\n",
      "Completeness Score:\n",
      "  Mean: 0.795\n",
      "  Median: 0.795\n",
      "  Std Dev: 0.037\n",
      "  Min: 0.768\n",
      "  Max: 0.821\n",
      "  Count: 2\n",
      "\n",
      "Correctness Score:\n",
      "  Mean: 0.782\n",
      "  Median: 0.782\n",
      "  Std Dev: 0.095\n",
      "  Min: 0.715\n",
      "  Max: 0.849\n",
      "  Count: 2\n",
      "\n",
      "Identification of Vulnerable Code Score:\n",
      "  Mean: 0.735\n",
      "  Median: 0.735\n",
      "  Std Dev: 0.092\n",
      "  Min: 0.670\n",
      "  Max: 0.800\n",
      "  Count: 2\n",
      "\n",
      "Code Guidance Score:\n",
      "  Mean: 0.843\n",
      "  Median: 0.843\n",
      "  Std Dev: 0.067\n",
      "  Min: 0.795\n",
      "  Max: 0.890\n",
      "  Count: 2\n"
     ]
    }
   ],
   "source": [
    "def detailed_metrics_analysis(df):\n",
    "    score_columns = [col for col in df.columns if col.endswith(' Score')]\n",
    "    \n",
    "    print(\"\\nDetailed Metrics Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in score_columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {df[col].mean():.3f}\")\n",
    "        print(f\"  Median: {df[col].median():.3f}\")\n",
    "        print(f\"  Std Dev: {df[col].std():.3f}\")\n",
    "        print(f\"  Min: {df[col].min():.3f}\")\n",
    "        print(f\"  Max: {df[col].max():.3f}\")\n",
    "        print(f\"  Count: {df[col].count()}\")\n",
    "\n",
    "\n",
    "detailed_metrics_analysis(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
