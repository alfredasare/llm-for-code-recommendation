{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import cuda\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from pinecone import Pinecone\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./bigvul.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23403"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup models and Pinecone connection\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize dense and sparse models\n",
    "dense_model = SentenceTransformer(\n",
    "    'msmarco-bert-base-dot-v5',\n",
    "    device=device\n",
    ")\n",
    "\n",
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "sparse_model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "# Initialize Pinecone\n",
    "pc = Pinecone(api_key='XXX')  # Replace with your actual API key\n",
    "mitre_index = pc.Index('metadata-aug-mitre')\n",
    "bigvul_index = pc.Index('metadata-retrieval-bigvul')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    \"\"\"Encode text into dense and sparse vectors for hybrid search.\"\"\"\n",
    "    try:\n",
    "        # Create dense vector\n",
    "        dense_vec = dense_model.encode(text).tolist()\n",
    "        \n",
    "        # Create sparse vector\n",
    "        input_ids = tokenizer(text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = sparse_model(**input_ids.to(device))\n",
    "            sparse_vec = torch.log(1 + torch.relu(outputs.logits)) * input_ids.attention_mask.unsqueeze(-1)\n",
    "            sparse_vec = torch.max(sparse_vec, dim=1)[0].squeeze()\n",
    "        \n",
    "        # Convert to dictionary format\n",
    "        indices = sparse_vec.nonzero().squeeze().cpu().tolist()\n",
    "        values = sparse_vec[indices].cpu().tolist()\n",
    "        sparse_dict = {\"indices\": indices, \"values\": values}\n",
    "        \n",
    "        return dense_vec, sparse_dict\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Encoding failed: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_mitre_index(cwe_id, top_k=1):\n",
    "    \"\"\"Query MITRE CWE index for weakness information.\"\"\"\n",
    "    query = f\"{cwe_id}\"\n",
    "    dense, sparse = encode(query)\n",
    "    \n",
    "    result = mitre_index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"cwe\": {\"$eq\": f\"{cwe_id}\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def query_bigvul_index(cve_id, cwe_id, top_k=5):\n",
    "    \"\"\"Query BigVul index for vulnerability information.\"\"\"\n",
    "    query = f\"Vulnerability: {cve_id} and Weakness: {cwe_id}\"\n",
    "    dense, sparse = encode(query)\n",
    "    \n",
    "    result = bigvul_index.query(\n",
    "        vector=dense,\n",
    "        sparse_vector=sparse,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter={\n",
    "            \"cwe\": {\"$eq\": cwe_id},\n",
    "            \"cve\": {\"$eq\": cve_id}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a software engineer and software vulnerability  expert who specializes in recommending fixes for vulnerable code affected by different CWEs and CVEs.\"),\n",
    "    (\"human\", \"\"\"\n",
    "        # CONTEXT #\n",
    "        You are a software engineer and software vulnerability expert who specializes in recommending fixes for vulnerable code affected by different CWEs and CVEs. This includes understanding the specific vulnerabilities and their potential impacts.\n",
    "    \n",
    "        # OBJECTIVE #\n",
    "        Your task is to recommend fixes for the provided vulnerable code. The recommendations should address the specific CWE in question and ensure that the code is secure against the identified vulnerabilities.\n",
    "    \n",
    "        # STYLE #\n",
    "        Write in a technical and concise manner, providing clear and actionable steps. \n",
    "    \n",
    "        # TONE #\n",
    "        Professional and technical.\n",
    "    \n",
    "        # AUDIENCE #\n",
    "        The target audience is software developers and security professionals who are looking to secure their code against known vulnerabilities.\n",
    "    \n",
    "        # RESPONSE FORMAT #\n",
    "        Provide a structured recommendation in the following format:\n",
    "        - Issue: [Brief description of the vulnerability]\n",
    "        - Recommendation: [Detailed steps to fix the vulnerability]\n",
    "        - Fix: [Code snippet demonstrating the fix]\n",
    "        \n",
    "    \n",
    "        Using this context that contains extra information and previous vulnerable code examples and fixes for the CWE in question, recommend how the vulnerable code can be fixed:\n",
    "    \n",
    "        Context:\n",
    "        {context}\n",
    "    \"\"\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to generate code fix recommendation using pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fix_recommendation(context, code):\n",
    "    combined_context = f\"\"\"\n",
    "        {context}\n",
    "\n",
    "        Vulnerable code to fix:\n",
    "        {code}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template.invoke({\"context\": combined_context})\n",
    "    result = llm.invoke(prompt)  \n",
    "\n",
    "    return result.content\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create context string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(cve_id, cwe_id):\n",
    "    \"\"\"Create context by querying both Pinecone indexes.\"\"\"\n",
    "    # Query MITRE CWE index for weakness information\n",
    "    try:\n",
    "        mitre_result = query_mitre_index(cwe_id)\n",
    "        cwe_context = \"\"\n",
    "        if mitre_result.matches:\n",
    "            cwe_metadata = mitre_result.matches[0].metadata\n",
    "            # Use actual metadata fields that exist\n",
    "            summary = cwe_metadata.get('Summary', '')\n",
    "            context_info = cwe_metadata.get('context', '')\n",
    "            cwe_context = f\"MITRE CWE Information:\\nSummary: {summary}\\nContext: {context_info}\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying MITRE index: {e}\")\n",
    "        cwe_context = \"\"\n",
    "\n",
    "    # Query BigVul index for vulnerability information\n",
    "    try:\n",
    "        bigvul_result = query_bigvul_index(cve_id, cwe_id)\n",
    "        cve_context = \"\"\n",
    "        if bigvul_result.matches:\n",
    "            # Combine metadata from all matches\n",
    "            all_summaries = []\n",
    "            all_contexts = []\n",
    "            all_vuln_fixes = []\n",
    "            for match in bigvul_result.matches:\n",
    "                metadata = match.metadata\n",
    "                summary = metadata.get('Summary', '')\n",
    "                context_info = metadata.get('context', '')\n",
    "                func_before = metadata.get('func_before', '')\n",
    "                func_after = metadata.get('func_after', '')\n",
    "                if summary:\n",
    "                    all_summaries.append(summary)\n",
    "                if context_info:\n",
    "                    all_contexts.append(context_info)\n",
    "                if func_before and func_after:\n",
    "                    all_vuln_fixes.append((func_before, func_after))\n",
    "            \n",
    "            cve_context = f\"BigVul Vulnerability Information:\\n\"\n",
    "            if all_summaries:\n",
    "                cve_context += f\"Summaries: {' | '.join(all_summaries)}\\n\"\n",
    "            if all_contexts:\n",
    "                cve_context += f\"Contexts: {' | '.join(all_contexts)}\\n\"\n",
    "            if all_vuln_fixes:\n",
    "                cve_context += f\"Vulnerability and Fix Examples:\\n\"\n",
    "                for i, (vuln_code, fixed_code) in enumerate(all_vuln_fixes, 1):\n",
    "                    cve_context += f\"Example {i}:\\n\"\n",
    "                    cve_context += f\"VULNERABLE CODE:\\n{vuln_code}\\n\\n\"\n",
    "                    cve_context += f\"FIXED CODE:\\n{fixed_code}\\n\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying BigVul index: {e}\")\n",
    "        cve_context = \"\"\n",
    "    \n",
    "    context = f\"Context:\\n{cwe_context}\\n{cve_context}\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Context:\\nMITRE CWE Information:\\nSummary: \\nContext: CWE ID: CWE-20\\n\\nBigVul Vulnerability Information:\\nSummaries: A missing check for popup window handling in Fullscreen in Google Chrome on macOS prior to 69.0.3497.81 allowed a remote attacker to spoof the contents of the Omnibox (URL bar) via a crafted HTML page. | A missing check for popup window handling in Fullscreen in Google Chrome on macOS prior to 69.0.3497.81 allowed a remote attacker to spoof the contents of the Omnibox (URL bar) via a crafted HTML page. | A missing check for popup window handling in Fullscreen in Google Chrome on macOS prior to 69.0.3497.81 allowed a remote attacker to spoof the contents of the Omnibox (URL bar) via a crafted HTML page. | A missing check for popup window handling in Fullscreen in Google Chrome on macOS prior to 69.0.3497.81 allowed a remote attacker to spoof the contents of the Omnibox (URL bar) via a crafted HTML page. | A missing check for popup window handling in Fullscreen in Google Chrome on macOS prior to 69.0.3497.81 allowed a remote attacker to spoof the contents of the Omnibox (URL bar) via a crafted HTML page.\\nContexts: Vulnerability: CVE-2018-16080\\nWeakness: CWE-20 | Vulnerability: CVE-2018-16080\\nWeakness: CWE-20 | Vulnerability: CVE-2018-16080\\nWeakness: CWE-20 | Vulnerability: CVE-2018-16080\\nWeakness: CWE-20 | Vulnerability: CVE-2018-16080\\nWeakness: CWE-20\\nVulnerability and Fix Examples:\\nExample 1:\\nVULNERABLE CODE:\\nbool BrowserView::IsIncognito() const {\\n  return browser_->profile()->IsOffTheRecord();\\n}\\n\\n\\nFIXED CODE:\\nbool BrowserView::IsIncognito() const {\\n  return browser_->profile()->IsOffTheRecord();\\n}\\n\\n\\nExample 2:\\nVULNERABLE CODE:\\nvoid BrowserView::OnWidgetMove() {\\n  if (!initialized_) {\\n    return;\\n  }\\n\\n\\n  if (status_bubble_.get())\\n    status_bubble_->Reposition();\\n\\n  BookmarkBubbleView::Hide();\\n\\n  LocationBarView* location_bar_view = GetLocationBarView();\\n  if (location_bar_view)\\n    location_bar_view->GetOmniboxView()->CloseOmniboxPopup();\\n}\\n\\n\\nFIXED CODE:\\nvoid BrowserView::OnWidgetMove() {\\n  if (!initialized_) {\\n    return;\\n  }\\n\\n\\n  if (status_bubble_.get())\\n    status_bubble_->Reposition();\\n\\n  BookmarkBubbleView::Hide();\\n\\n  LocationBarView* location_bar_view = GetLocationBarView();\\n  if (location_bar_view)\\n    location_bar_view->GetOmniboxView()->CloseOmniboxPopup();\\n}\\n\\n\\nExample 3:\\nVULNERABLE CODE:\\ngfx::ImageSkia BrowserView::GetWindowAppIcon() {\\n  extensions::HostedAppBrowserController* app_controller =\\n      browser()->hosted_app_controller();\\n  return app_controller ? app_controller->GetWindowAppIcon() : GetWindowIcon();\\n}\\n\\n\\nFIXED CODE:\\ngfx::ImageSkia BrowserView::GetWindowAppIcon() {\\n  extensions::HostedAppBrowserController* app_controller =\\n      browser()->hosted_app_controller();\\n  return app_controller ? app_controller->GetWindowAppIcon() : GetWindowIcon();\\n}\\n\\n\\nExample 4:\\nVULNERABLE CODE:\\nstd::string TestBrowserWindow::GetWorkspace() const {\\n  return std::string();\\n}\\n\\n\\nFIXED CODE:\\nstd::string TestBrowserWindow::GetWorkspace() const {\\n  return std::string();\\n}\\n\\n\\nExample 5:\\nVULNERABLE CODE:\\nvoid BrowserView::TabDetachedAt(WebContents* contents,\\n                                int index,\\n                                bool was_active) {\\n  if (was_active) {\\n    web_contents_close_handler_->ActiveTabChanged();\\n    contents_web_view_->SetWebContents(nullptr);\\n    infobar_container_->ChangeInfoBarManager(nullptr);\\n    UpdateDevToolsForContents(nullptr, true);\\n  }\\n}\\n\\n\\nFIXED CODE:\\nvoid BrowserView::TabDetachedAt(WebContents* contents,\\n                                int index,\\n                                bool was_active) {\\n  if (was_active) {\\n    web_contents_close_handler_->ActiveTabChanged();\\n    contents_web_view_->SetWebContents(nullptr);\\n    infobar_container_->ChangeInfoBarManager(nullptr);\\n    UpdateDevToolsForContents(nullptr, true);\\n  }\\n}\\n\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_context(test_df['CVE ID'][1], test_df['CWE ID'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load progress if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file = './output/recommendations.csv'\n",
    "if os.path.exists(progress_file):\n",
    "    results_df = pd.read_csv(progress_file)\n",
    "    processed_ids = set(results_df['cve'].astype(str) + \"_\" + results_df['cwe'].astype(str))\n",
    "    results = results_df.to_dict('records')\n",
    "else:\n",
    "    results = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterate through the test dataset and generate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-12714_CWE-787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 1/2 [00:15<00:15, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-16541_CWE-416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2/2 [00:29<00:00, 14.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(test_df.head(2).iterrows(), total=test_df.head(2).shape[0], desc=\"Processing\"):\n",
    "    current_id = f\"{row['CVE ID']}_{row['CWE ID']}\"\n",
    "\n",
    "    # Skip already processed examples\n",
    "    if current_id in processed_ids:\n",
    "        continue\n",
    "\n",
    "    # Create context string\n",
    "    context = create_context(row['CVE ID'], row['CWE ID'])\n",
    "\n",
    "    print(f\"Running for {current_id}\")\n",
    "\n",
    "    func_before = row['func_before']\n",
    "    \n",
    "    # Generate fixes\n",
    "    recommendation = generate_fix_recommendation(context, func_before)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'cve': row['CVE ID'],\n",
    "        'cwe': row['CWE ID'],\n",
    "        'context': context,\n",
    "        'func_after': row['func_after'],\n",
    "        'func_before': row['func_before'],\n",
    "        'recommendation': recommendation,\n",
    "    })\n",
    "\n",
    "    # Save progress every 10 examples\n",
    "    if index % 1 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(progress_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Fixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load recommendation\n",
    "recommendations = pd.read_csv('./output/recommendations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixes Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_messages = [\n",
    "    (\"system\", \"You are a software engineer and security expert who specializes in generating fixes for vulnerable code affected by different CWEs and CVEs.\"),\n",
    "    (\"human\", \"\"\"\n",
    "        # CONTEXT #\n",
    "        You are a software engineer and security expert who specializes in generating fixes for vulnerable code affected by different CWEs and CVEs.\n",
    "        \n",
    "        # OBJECTIVE #\n",
    "        Generate a fix for the given vulnerable code based on the provided context.\n",
    "        \n",
    "        # STYLE #\n",
    "        Provide the fixed code snippet only, following best practices for secure and efficient coding.\n",
    "        \n",
    "        # TONE #\n",
    "        Professional and technical.\n",
    "        \n",
    "        # AUDIENCE #\n",
    "        Software engineers and security experts.\n",
    "        \n",
    "        # RESPONSE FORMAT #\n",
    "        The response should be a single corrected code snippet without any additional explanations or comments.\n",
    "        \n",
    "        # PROMPT #\n",
    "        Based on the following vulnerable code and the given recommendation, generate a fixed version of the code:\n",
    "        {context}\n",
    "    \"\"\"),\n",
    "]\n",
    "\n",
    "fixes_prompt_template = ChatPromptTemplate.from_messages(fix_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fix(code_before, recommendation, cve, cwe):\n",
    "    combined_context = f\"\"\"\n",
    "        Vulnerable code:\n",
    "        CWE: {cwe}\n",
    "        CVE: {cve}\n",
    "\n",
    "        Code:\n",
    "        {code_before}\n",
    "\n",
    "        Recommendation:\n",
    "        {recommendation}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = fixes_prompt_template.invoke({\"context\": combined_context})\n",
    "    result = llm.invoke(prompt)\n",
    "\n",
    "    return result.content\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file = './output/fixes.csv'\n",
    "if os.path.exists(progress_file):\n",
    "    results_df = pd.read_csv(progress_file)\n",
    "    processed_ids = set(results_df['cve'].astype(str) + \"_\" + results_df['cwe'].astype(str))\n",
    "    results = results_df.to_dict('records')\n",
    "else:\n",
    "    results = []\n",
    "    processed_ids = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-12714_CWE-787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for CVE-2018-16541_CWE-416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(recommendations.iterrows(), total=recommendations.shape[0], desc=\"Processing\"):\n",
    "    current_id = f\"{row['cve']}_{row['cwe']}\"\n",
    "\n",
    "    # Skip already processed examples\n",
    "    if current_id in processed_ids:\n",
    "        continue\n",
    "\n",
    "    print(f\"Running for {current_id}\")\n",
    "\n",
    "    code_before = row['func_before']\n",
    "    \n",
    "    # Generate fixes\n",
    "    fix = generate_fix(code_before, row['recommendation'], row['cve'], row['cwe'])\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'cve': row['cve'],\n",
    "        'cwe': row['cwe'],\n",
    "        'context': row['context'],\n",
    "        'func_after': row['func_after'],\n",
    "        'func_before': row['func_before'],\n",
    "        'recommendation': row['recommendation'],\n",
    "        'fix': fix,\n",
    "    })\n",
    "\n",
    "    # Save progress every 10 examples\n",
    "    if index % 1 == 0:\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_csv(progress_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Judge Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubrics = [\n",
    "    {\n",
    "        \"name\": \"Relevance\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is relevant to the identified vulnerability in both the context and the provided code snippet, and whether the suggested fix is applicable and practical within the given scenario.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Verify if the 'actual output' (recommendation) directly addresses the specific vulnerability identified in the 'input' (context), including CWE, CVE, and details from the code snippet.\",\n",
    "            \"Ensure the 'actual output' provides a solution that is applicable to both the provided context (CWE/CVE information) and the code snippet and can realistically be implemented in the given scenario.\",\n",
    "            \"Assess whether the recommendation avoids irrelevant information or suggestions that do not align with the vulnerability details or coding context.\",\n",
    "            \"Determine if the recommendation includes practical and actionable steps that align with the specific characteristics of the vulnerability, such as the type of flaw and its occurrence within the code.\",\n",
    "            \"Penalize any 'actual output' that is generic, lacks relevance to both the context and the code provided, or suggests impractical or inapplicable solutions.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Completeness\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is complete and thorough in addressing the identified vulnerability, covering all necessary steps and details required to implement the fix effectively, with reference to both the context and the code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) covers all necessary steps to fully address the vulnerability described in the 'input' (context), including CWE, CVE, and the code snippet.\",\n",
    "            \"Ensure that the recommendation provides detailed explanations and instructions for each step, leaving no ambiguity about how to implement the fix, referencing both the vulnerable code and the retrieved context.\",\n",
    "            \"Evaluate whether the recommendation considers all relevant aspects of the vulnerability, including its nature, impact, and potential variations in different coding contexts.\",\n",
    "            \"Identify any missing steps or details in the 'actual output' that could lead to incomplete implementation or residual vulnerabilities.\",\n",
    "            \"Penalize any 'actual output' that lacks comprehensiveness, omits critical details, or provides insufficient guidance for implementing a complete fix.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Correctness\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) is factually correct, technically accurate, and free of errors or misleading information, based on the expected output, known standards, and the provided context and code snippet for addressing the identified vulnerability.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) aligns with the correct approach to fixing the vulnerability described in the 'input' (context), including CWE, CVE, and the code snippet.\",\n",
    "            \"Verify the technical accuracy of each step in the 'actual output', ensuring they are based on sound principles, the provided code, and best practices for security fixes.\",\n",
    "            \"Identify any factual inaccuracies, technical errors, or misleading information in the 'actual output' that could lead to incorrect implementation or further vulnerabilities.\",\n",
    "            \"Evaluate if the 'actual output' correctly interprets the vulnerability details (CWE, CVE) and provides an appropriate solution based on known standards and methodologies.\",\n",
    "            \"Penalize any 'actual output' that contains factual inaccuracies, incorrect technical guidance, or misleading statements.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Identification of Vulnerable Code\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) accurately identifies the specific parts of the code that are vulnerable based on the provided context, including CWE, CVE, and the code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) clearly identifies the specific lines, functions, or code snippets in the 'input' (context) that are vulnerable.\",\n",
    "            \"Ensure the 'actual output' pinpoints the exact location of the vulnerability in the code snippet, referencing relevant details such as line numbers or specific code segments.\",\n",
    "            \"Assess whether the recommendation provides enough detail to locate the vulnerable code without ambiguity or confusion.\",\n",
    "            \"Evaluate if the 'actual output' includes all relevant parts of the code that are vulnerable, without missing any significant areas.\",\n",
    "            \"Penalize any 'actual output' that fails to identify the vulnerable code clearly, is too vague, or misidentifies non-vulnerable code as vulnerable.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Guidance\",\n",
    "        \"criteria\": \"Evaluate whether the actual output (recommendation) provides relevant, actionable code snippets and clear guidance to effectively address the identified vulnerability based on the provided context, including CWE, CVE, and code snippet.\",\n",
    "        \"evaluation_steps\": [\n",
    "            \"Check if the 'actual output' (recommendation) includes relevant and actionable code snippets that directly address the vulnerability described in the 'input' (context).\",\n",
    "            \"Ensure that the code snippets provided are syntactically correct, follow best practices, and are applicable to the coding environment specified in the context.\",\n",
    "            \"Assess whether the 'actual output' provides clear, step-by-step guidance on how to implement the code changes to fix the vulnerability.\",\n",
    "            \"Evaluate if the recommendation includes sufficient detail in the code snippets and guidance to be easily implemented by a developer without needing additional information.\",\n",
    "            \"Penalize any 'actual output' that lacks relevant code snippets, provides vague or incorrect guidance, or includes incomplete or confusing instructions.\",\n",
    "            \"Penalize heavily if the CVE and CWE mentioned in the 'input' context do not match or align with the content of the recommendation.\"\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "fixes_data = pd.read_csv('./output/fixes.csv')\n",
    "\n",
    "for index, row in fixes_data.iterrows():\n",
    "    context = create_context(row['cve'], row['cwe'])\n",
    "    \n",
    "    sample = {\n",
    "        \"cwe\": row['cwe'],\n",
    "        \"cve\": row['cve'],\n",
    "        \"code_snippet\": f\"### Vulnerable Code\\n{row['func_before']}\\n\\n\\n### Vulnerable Code Fix\\n{row['func_after']}\\n\", \n",
    "        \"context\": row['context'],\n",
    "        \"recommendation\": row['recommendation']\n",
    "    }\n",
    "    data.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_text(cwe, cve, code_snippet, context):\n",
    "    return f\"\"\"\n",
    "        ### CVE-ID\n",
    "        {cve}\n",
    "\n",
    "        ### CWE-ID\n",
    "        {cwe}\n",
    "    \n",
    "        ### Vulnerable Code and Fix\n",
    "        {code_snippet}\n",
    "\n",
    "\n",
    "        ### Retrieved Data from Vector DB\n",
    "        {context}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = \"./output/evaluation.csv\"\n",
    "if os.path.exists(checkpoint_file):\n",
    "    df = pd.read_csv(checkpoint_file)\n",
    "    processed_keys = set(df[\"cwe_cve_idx\"])  # Track already processed cwe_cve_idx\n",
    "    results = df.to_dict('records')\n",
    "else:\n",
    "    processed_keys = set()\n",
    "    results = []\n",
    "\n",
    "def save_checkpoint(results, checkpoint_file):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(checkpoint_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, example in enumerate(tqdm(data, desc=\"Processing Examples\")):\n",
    "    cwe_cve_idx = f\"{example['cwe']}_{example['cve']}_{idx}\"  \n",
    "\n",
    "    if cwe_cve_idx in processed_keys:\n",
    "        continue \n",
    "\n",
    "    print(f\"Processing index {idx}: CWE/CVE {cwe_cve_idx}\")\n",
    "\n",
    "    example_results = {\n",
    "        \"cwe\": example[\"cwe\"],\n",
    "        \"cve\": example[\"cve\"],\n",
    "        \"cwe_cve_idx\": cwe_cve_idx,\n",
    "        \"recommendation\": example[\"recommendation\"],\n",
    "        \"code_snippet\": example[\"code_snippet\"],\n",
    "        \"context\": example[\"context\"]\n",
    "    }\n",
    "\n",
    "    for rubric in rubrics:\n",
    "        g_eval = GEval(\n",
    "            **rubric,\n",
    "            model=\"gpt-4o\",\n",
    "            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]\n",
    "        )\n",
    "\n",
    "        test_case = LLMTestCase(\n",
    "            input=create_input_text(example[\"cwe\"], example[\"cve\"], example[\"code_snippet\"], example[\"context\"]),\n",
    "            actual_output=example[\"recommendation\"]\n",
    "        )\n",
    "\n",
    "        g_eval.measure(test_case)\n",
    "        example_results[f\"{rubric['name']} Score\"] = float(g_eval.score)\n",
    "\n",
    "    results.append(example_results)\n",
    "    processed_keys.add(cwe_cve_idx)\n",
    "\n",
    "    # Save checkpoint after each example is processed\n",
    "    save_checkpoint(results, checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(checkpoint_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Scores for All Metrics:\n",
      "========================================\n",
      "Relevance Score: 0.844\n",
      "Completeness Score: 0.795\n",
      "Correctness Score: 0.782\n",
      "Identification of Vulnerable Code Score: 0.735\n",
      "Code Guidance Score: 0.843\n",
      "========================================\n",
      "Overall Average: 0.800\n"
     ]
    }
   ],
   "source": [
    "# Calculate average scores for all metrics\n",
    "def calculate_average_metrics(df):\n",
    "    score_columns = [col for col in df.columns if col.endswith(' Score')]\n",
    "    \n",
    "    print(\"Average Scores for All Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    averages = {}\n",
    "    for col in score_columns:\n",
    "        avg_score = df[col].mean()\n",
    "        averages[col] = avg_score\n",
    "        print(f\"{col}: {avg_score:.3f}\")\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Overall Average: {sum(averages.values()) / len(averages):.3f}\")\n",
    "    \n",
    "    return averages\n",
    "\n",
    "averages = calculate_average_metrics(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Metrics Analysis:\n",
      "============================================================\n",
      "\n",
      "Relevance Score:\n",
      "  Mean: 0.844\n",
      "  Median: 0.844\n",
      "  Std Dev: 0.043\n",
      "  Min: 0.813\n",
      "  Max: 0.874\n",
      "  Count: 2\n",
      "\n",
      "Completeness Score:\n",
      "  Mean: 0.795\n",
      "  Median: 0.795\n",
      "  Std Dev: 0.037\n",
      "  Min: 0.768\n",
      "  Max: 0.821\n",
      "  Count: 2\n",
      "\n",
      "Correctness Score:\n",
      "  Mean: 0.782\n",
      "  Median: 0.782\n",
      "  Std Dev: 0.095\n",
      "  Min: 0.715\n",
      "  Max: 0.849\n",
      "  Count: 2\n",
      "\n",
      "Identification of Vulnerable Code Score:\n",
      "  Mean: 0.735\n",
      "  Median: 0.735\n",
      "  Std Dev: 0.092\n",
      "  Min: 0.670\n",
      "  Max: 0.800\n",
      "  Count: 2\n",
      "\n",
      "Code Guidance Score:\n",
      "  Mean: 0.843\n",
      "  Median: 0.843\n",
      "  Std Dev: 0.067\n",
      "  Min: 0.795\n",
      "  Max: 0.890\n",
      "  Count: 2\n"
     ]
    }
   ],
   "source": [
    "def detailed_metrics_analysis(df):\n",
    "    score_columns = [col for col in df.columns if col.endswith(' Score')]\n",
    "    \n",
    "    print(\"\\nDetailed Metrics Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for col in score_columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {df[col].mean():.3f}\")\n",
    "        print(f\"  Median: {df[col].median():.3f}\")\n",
    "        print(f\"  Std Dev: {df[col].std():.3f}\")\n",
    "        print(f\"  Min: {df[col].min():.3f}\")\n",
    "        print(f\"  Max: {df[col].max():.3f}\")\n",
    "        print(f\"  Count: {df[col].count()}\")\n",
    "\n",
    "\n",
    "detailed_metrics_analysis(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
